{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ba6b5e-4c97-4ae5-ae20-4c970d6ef1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1feb836-2f5a-4b1b-b05a-9092c50c1651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from TransUNet.utils import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a7db3d-b493-4e7c-b8a1-1cfdad2d6df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2, ast\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageFilter\n",
    "import pydicom\n",
    "from skimage import measure\n",
    "from scipy.spatial import ConvexHull\n",
    "from PIL import Image, ImageDraw\n",
    "import copy\n",
    "from scipy.ndimage import rotate\n",
    "import random\n",
    "import seaborn as sns\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb08295d-4b64-4f88-b094-f3d108971433",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label1</th>\n",
       "      <th>mask</th>\n",
       "      <th>hu_array</th>\n",
       "      <th>hu_array_old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR2</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR2</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR2</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR2</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR2</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...</td>\n",
       "      <td>[[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label1                                               mask  \\\n",
       "0    LR2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1    LR2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2    LR2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3    LR2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4    LR2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                            hu_array  \\\n",
       "0  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "1  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "2  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "3  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "4  [[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0...   \n",
       "\n",
       "                                        hu_array_old  \n",
       "0  [[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...  \n",
       "1  [[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...  \n",
       "2  [[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...  \n",
       "3  [[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...  \n",
       "4  [[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_pickle('lung_cancer_test.pkl')\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e897679-59f5-41a3-93ea-ba685a2c730b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from TransUNet.networks.vit_seg_modeling import VisionTransformer as ViT_seg\n",
    "from TransUNet.networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n",
    "config_vit = CONFIGS_ViT_seg['R50-ViT-L_16']\n",
    "config_vit.n_classes = 2\n",
    "config_vit.n_skip = 3\n",
    "img_size = 512\n",
    "vit_patches_size = 16\n",
    "config_vit.patches.grid = (int(img_size / vit_patches_size), int(img_size / vit_patches_size))\n",
    "net = ViT_seg(config_vit, img_size=img_size, num_classes=config_vit.n_classes).cuda()\n",
    "#net.load_from(weights=np.load(config_vit.pretrained_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ab2bb6-b2b5-4811-a037-7a5c3ffc1eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('U-Net with VGG/trans-u-net-R50-base-512_withdataaugn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bf96c9-f859-427b-a436-e0c82bb29ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (embeddings): Embeddings(\n",
       "      (hybrid_model): ResNetV2(\n",
       "        (root): Sequential(\n",
       "          (conv): StdConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (gn): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (body): Sequential(\n",
       "          (block1): Sequential(\n",
       "            (unit1): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn_proj): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (unit2): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit3): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (unit1): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): StdConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (gn_proj): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (unit2): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit3): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit4): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (block3): Sequential(\n",
       "            (unit1): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): StdConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (gn_proj): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (unit2): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit3): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit4): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit5): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit6): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit7): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit8): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit9): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (patch_embeddings): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (12): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (13): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (14): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (15): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (16): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (17): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (18): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (19): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (20): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (21): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (22): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (23): Block(\n",
       "          (attention_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderCup(\n",
       "    (conv_more): Conv2dReLU(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8730e4f5-973d-4f54-9aed-2d2c4dbe9c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_dice_coefficient(mask_true, mask_pred):\n",
    "    intersection = np.sum(mask_true * mask_pred)\n",
    "    union = np.sum(mask_true) + np.sum(mask_pred)    \n",
    "    dice_coefficient = (2.0 * intersection) / (union + 1e-8)     \n",
    "    return dice_coefficient\n",
    "def calculate_iou(mask_true, mask_pred):\n",
    "    intersection = np.sum(mask_true * mask_pred)\n",
    "    union = np.sum(mask_true) + np.sum(mask_pred) - intersection   \n",
    "    iou = (intersection + 1e-8) / (union + 1e-8)     \n",
    "    return iou\n",
    "def calculate_iou(mask_true, mask_pred):\n",
    "    intersection = np.sum(mask_true * mask_pred)\n",
    "    union = np.sum(mask_true) + np.sum(mask_pred) - intersection   \n",
    "    iou = (intersection + 1e-8) / (union + 1e-8)      \n",
    "    return iou\n",
    "def precision_score(groundtruth_mask, pred_mask):\n",
    "    true_positives = np.sum(pred_mask * groundtruth_mask)\n",
    "    total_pixel_pred = np.sum(pred_mask)\n",
    "    precision = (true_positives + 1e-8) / (total_pixel_pred + 1e-8)\n",
    "    return precision\n",
    "def recall_score(groundtruth_mask, pred_mask):\n",
    "    true_positives = np.sum(pred_mask * groundtruth_mask)\n",
    "    total_pixel_truth = np.sum(groundtruth_mask)\n",
    "    recall = (true_positives + 1e-8) / (total_pixel_truth + 1e-8)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0747f766-5df3-4fb5-b5be-5edca04f3a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice for LR2 0.10003961098749836 35\n",
      "Dice for LR3 0.37832149072381266 37\n",
      "Dice for LR4A 0.45291734407963113 97\n",
      "Dice for LR4B 0.5889898933686388 95\n",
      "Average\n",
      "DICE 0.4446450901026077\n",
      "IoU 0.39008079215403046\n",
      "precision 0.8546991933575012\n",
      "recall 0.43647317457704454\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "d = []\n",
    "iou = []\n",
    "prec = []\n",
    "rec = []\n",
    "labels = val_df['label1'].unique()\n",
    "for j in labels:\n",
    "    dices = []\n",
    "    test = val_df.loc[val_df['label1'] == j].reset_index(drop=True)\n",
    "    for i in range(len(test)):\n",
    "        img = test['hu_array'][i]\n",
    "        img = (img-np.min(img))/(np.max(img)-np.min(img))\n",
    "        img = exposure.equalize_adapthist(img/np.max(img))\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img = img.float().cuda()  \n",
    "        out = net(img.unsqueeze(0))  \n",
    "        out = torch.softmax(out, dim=1)\n",
    "        mask = test['mask'][i]   \n",
    "        masks = (mask > 0.5).astype(int)\n",
    "        try:\n",
    "            mask_pred = (out[0][1].cpu().detach().numpy() > 0.5)**2\n",
    "        except:\n",
    "            mask_pred = np.zeros([512,512])\n",
    "        dices.append(calculate_dice_coefficient(masks, mask_pred))\n",
    "        d.append(calculate_dice_coefficient(masks, mask_pred))\n",
    "        iou.append(calculate_iou(masks, mask_pred))\n",
    "        prec.append(precision_score(masks, mask_pred))\n",
    "        rec.append(recall_score(masks, mask_pred))\n",
    "    print(\"Dice for \" + j + \" \" + str(np.mean(dices))+\" \" +str(len(dices)))\n",
    "print(\"Average\") \n",
    "print(\"DICE \" + str(np.mean(d)))\n",
    "print(\"IoU \" + str(np.mean(iou)))\n",
    "print(\"precision \" + str(np.mean(prec)))\n",
    "print(\"recall \"+str(np.mean(rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3fe483c-dd66-4664-81c1-f4a0749d8849",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324871586"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-unet",
   "language": "python",
   "name": "trans-unet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
